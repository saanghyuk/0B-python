{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(y|x) : 확률적 모형\n",
    "    - 확률적 판별 모형 : f(x)=P(y|x)로 놓고 함수를 바로 찾아내는 것. \n",
    "    - 확률적 생성 모형 : likelihood P(x|y)를 먼저 찾은 후, 베이즈 정리로 P(y|x)를 추적하는 방식이 있었음. \n",
    "        - 근데 왜 이렇게 복잡하게 하지? \n",
    "        - 나이브가정을 사용할 수 있다. $f(x)=f(x_1)f(x_2)...f(x_D)$. 독립변수가 많을 때 이렇게 나누는 것이 도움이 된다. \n",
    "        - 그래서 **변수가 많을 때 주로 쓴다.**\n",
    "        \n",
    "- 지금부터 나오는 방법은 확률을 쓰지 않는 방식. \n",
    "    - 퍼셉트론 : 단순한 모형이지만, Deep Learning의 기초이기 떄문에 중요하다. \n",
    "    - 서포트벡터머신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론(perceptron)은 가장 오래되고 단순한 형태의 판별함수기반 분류모형 중 하나이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 상수항 추가된 독립변수 벡터가 있다. \n",
    "2. 가중치 마스크를 씌운다. 가중치와 내적. 스칼라가 된다. \n",
    "3. 그 스칼라의 부호를 가지고 판단을 한다. +면, class 1, -면 class 0 이런식. \n",
    "\n",
    "\n",
    "그럼 이제 중요한 것은 $w$를 어떻게 찾을 것인가? 찾을때, prediction한 값과 실제 값 사이의 loss function을 구한다. 그리고 그 loss function을 미분한 gradient vector를 찾아내고 그 gradient vector만큼 가중치를 갱신해 가는 방식이다. <br>\n",
    "\n",
    "판별함수 모형에서는 $\\hat{y}$을 쓸때 {+1, -1}을 주로 쓴다. 앞에서는 +1, 0을 주로 썼었지. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_99.png](./materials/1_99.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_100-2.png](./materials/1_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "판별선 경계선은 똑같이 직선이다. 선형조합이니깐. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서는 이 부호함수만 logistic/tangent 이렇게 바뀌는 것. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y는 +1, -1만 가질 수 있다는 것 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_116.jpeg](./materials/1_116.jpeg)\n",
    "![1_116.jpeg](./materials/1_117.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_118.jpeg](./materials/1_118.jpeg)\n",
    "![1_118.jpeg](./materials/1_119.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_120.jpeg](./materials/1_120.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_100-2.png](./materials/1_101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별적 데이터 하나하나에 대해 손실함수를 통해 손실을 구하고, 그것을 총합을 한 것이 전체손실함수 값. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_100-2.png](./materials/1_102.png)\n",
    "![1_100-2.png](./materials/1_103.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론 손실함수 $L_p(w)$를 최소화하는 $w$를 찾기 위해 $L_p(w)$를 $w$로 미분하여 그레디언트를 구하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_104.png](./materials/1_104.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수렴이 늦어질 뿐이지, 언젠가는 수렴이 된다는 것이 수학적으로 증명이 되어 있다. 실제로는 그렇게 많이 느려지지도 않는다. <br>\n",
    "틀린데이터를 다 쓰면, 거의 항상 매번 Step 마다 Loss가 내려간다. <br>\n",
    "그런데, Stochastic쓰면 올라갔다 내려갔다 하면서, 수렴지점으로 따라간다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn 의 퍼셉트론 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn에서 제공하는 퍼셉트론모형인 Perceptron 클래스는 다음과 같은 입력인수를 가진다.\n",
    "- max_iter : 최적화를 위한 반복 횟수(iteration number)\n",
    "- eta0 : 학습속도\n",
    "- n_iter_no_change : 이 설정값만큼 반복을 해도 성능이 나아지지 않으면 max_iter 설정값과 상관없이 멈춘다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이거 코드 실행시켜보면, 영상이 나온다. 계속 왔다갔다 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_105.png](./materials/1_105.png)\n",
    "![1_105.png](./materials/1_106.png)\n",
    "![1_105.png](./materials/1_107.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_108.png](./materials/1_108.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stoachastic방법을 쓰니깐, 단조함수적으로 줄어드는게 아니라 와리가리 치면서 아래 방향으로 향해간다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD(Stochastic Gradient Descent) 방법은 손실함수 자체가 아니라 손실함수의 기댓값을 최소하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 말한, Stochastic Gradient Descent는 딱 랜덤하게 틀린것 중에 하나만 사용해서 w를 업데이트 했음.<br>\n",
    "미니배치라고 부르는, 일부데이터를 사용해서 gradient의 추정치를 구해서 w값 업데이트하는 방식.<br>\n",
    "아래 나오는 기댓값의 추정치라는 것이 별게 아니고, 틀린 데이터중에 mini-batch 수 만큼 뽑아서, 걔네들만 가지고 평균내서 그걸로 w 업데이트 한다는 것. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_109.png](./materials/1_109.png)\n",
    "![1_109.png](./materials/1_110.png)\n",
    "![1_109.png](./materials/1_111.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많이 쓰는 것은, **제곱힌지**와 **수정휴버** 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn 의 SGD 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn에서제공하는 SGDClassifier 클래스는 Perceptron 클래스에의 입력인수 이외에도 손실함수를 결정하는 loss 인수를 가진다. \n",
    "가능합 값은 \n",
    "- hinge, perceptron, log, huber, modified_huber, squared_hinge 등이다. \n",
    "- 보통 modified_huber 를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_112.png](./materials/1_112.png)\n",
    "![1_112.png](./materials/1_113.png)\n",
    "![1_112.png](./materials/1_114.png)\n",
    "![1_112.png](./materials/1_115.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중요한 포인트\n",
    "\n",
    "\n",
    "그런데, 가장 나쁜 점 중에 하나는. 0이 된 다음에 수렴이 끝나야되는데, <br>\n",
    "원래 gradient descent에서는 최적점에 도달하면 0이 나와야되는데, <br>\n",
    "애초에 지금 쓰는 방식이 정확한 gradient descent를 구한게 아니잖아. <br>\n",
    "그래서, 최적점에 가도 딱 0이 안나올 수도 있어. <br>\n",
    "\n",
    "때문에, 최적점갔다가 다시 튕겨나오는 경우가 많다. <br>\n",
    "그래서, 딥러닝이든 퍼셉트론이든 stochastic gradient descent에서는 우리가 이 loss를 보고 있어야돼. <br>\n",
    "수렴 한거 같으면 수동으로 정지시켜야돼. <br>\n",
    "딥러닝도 마찬가지. <br>\n",
    "그래서, 모니터링이 중요한 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
