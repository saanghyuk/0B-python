{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그레디언트 소멸문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로도 레이어를 복잡하게 하면 변환함수가 복잡해 지고 복잡한 데이터도 잘 모델링이 된다. <br>\n",
    "현실적으로도 많이 씀. <br>\n",
    "그런데, w/b를 찾는 optimization과정이 점점 느려지는 현상이 발생한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 레이어의 수가 많을수록 복잡한 형태의 베이시스 함수를 만들 수 있다. 하지만, 레이어 수가 증가하면 가중치가 잘 수렴하지 않는 현상이 발생한다. 원인을 알아보기 위해 학습 중에 가중치가 어떻게 변화하는지 알아보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 MNIST Digit Image 데이터를 사용한다. 여기에서는 교차검증은 하지 않을 것이므로 학습용 데이터만 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n",
    "\n",
    "X_train = X_train0.reshape(60000, 784).astype('float32') / 255.0\n",
    "y_train = np_utils.to_categorical(y_train0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비교를 위해 4개의 신경망 모형을 keras로 만든다. 모형1은 하나의 히든레이어를 가지는 모형이다. 모형2, 모형3, 모형4는 각각 2개, 3개, 4개의 히든레이어를 가진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model1.add(Dense(10, activation=\"sigmoid\"))\n",
    "model1.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model2.add(Dense(15, activation=\"sigmoid\"))\n",
    "model2.add(Dense(10, activation=\"sigmoid\"))\n",
    "model2.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model3.add(Dense(15, activation=\"sigmoid\"))\n",
    "model3.add(Dense(15, activation=\"sigmoid\"))\n",
    "model3.add(Dense(10, activation=\"sigmoid\"))\n",
    "model3.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(10, activation=\"sigmoid\"))\n",
    "model4.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 실험에서는 학습 중에 가중치가 얼마나 빠르게 변하는지를 보기 위해 가중치 변화(절대값)의 평균을 한 세대(epoch)마다 기록할 것이다. 이를 위해 keras의 `Callback` 클래스를 사용한다. `Callback` 클래스는 하나의 세대가 끝날 때 마다 `on_epoch_end`라는 메서드를 호출하게 된다. 이 메서드에서 각 히든레이어의 가중치 변화의 평균을 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_37.jpeg](./materials/3_37.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_38.jpeg](./materials/3_38.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어쨋든 공통적으로 앞에 있는 레이어는 안변한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 예측 성능 및 수렴 성능을 개선하기 위해서는 다음과 같은 추가적인 고려를 해야 한다. \n",
    "- 크로스 엔트로피 오차함수 \n",
    "- hyper-tangent and ReLu 활성화 함수 \n",
    "- 가중치 초기화 정규값\n",
    "- Drop-out 정규화 \n",
    "- Softmax 출력 : 원래는 각자 마지막 출력노드가 각자 판별함수라 꼭 1이 안나옴. 합해서 1이 나오게 해 주는게 소프트맥스. 확률처럼 보기좋게 해주려고 달아주는 것. \n",
    "- 배치 정규화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_38.jpeg](./materials/3_39.jpeg)\n",
    "![3_38.jpeg](./materials/3_40.jpeg)\n",
    "![3_38.jpeg](./materials/3_41.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 최적화 방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_42.jpeg](./materials/3_42.jpeg)\n",
    "![3_42.jpeg](./materials/3_43.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
