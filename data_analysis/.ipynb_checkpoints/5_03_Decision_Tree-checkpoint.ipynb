{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression, Classification에서 둘다 쓸 수 있음. <br>\n",
    "그래서, CART라고도 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "독립변수 중에 딱 하나만 고른다. 그리고, 그 독립변수에 대한 기준값을 구해서 데이터를 나눈다. <br>\n",
    "데이터 처음에 100개 있었으면, $x_3<5$ 기준으로 YES/NO로 분류. <br>\n",
    "그러면 데이터 나뉘겠지. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_79.jpeg](./materials/1_79.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**의사결정나무(decision tree)**는 여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다. 분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 **CART(Classification And Regression Tree)**라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무를 이용한 분류학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무를 이용한 분류법은 다음과 같다.\n",
    "\n",
    "1. 여러가지 독립 변수 중 하나의 독립 변수를 선택하고 그 독립 변수에 대한 기준값(threshold)을 정한다. 이를 분류 규칙이라고 한다. 최적의 분류 규칙을 찾는 방법은 이후에 자세히 설명한다.\n",
    "2. 전체 학습 데이터 집합(부모 노드)을 해당 독립 변수의 값이 기준값보다 작은 데이터 그룹(자식 노드 1)과 해당 독립 변수의 값이 기준값보다 큰 데이터 그룹(자식 노드 2)으로 나눈다. \n",
    "3. 각각의 자식 노드에 대해 1~2의 단계를 반복하여 하위의 자식 노드를 만든다. 단, 자식 노드에 한가지 클래스의 데이터만 존재한다면 더 이상 자식 노드를 나누지 않고 중지한다.\n",
    "\n",
    "이렇게 자식 노드 나누기를 연속적으로 적용하면 노드가 계속 증가하는 나무(tree)와 같은 형태로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 모든 노드에 다 순수한 하나의 class만 남으면 좋은데, 꼭 다 그렇게 되지는 않겠지. <br>\n",
    "해도해도 안끝나는 경우가 있겠지. <br>\n",
    "자식노드 1씩 내려갈때마다 깊이가 1씩 깊어진다고 말하는데, 깊이를 제한해버리는 경우가 많다. <br>\n",
    "제한 하면, 순수하게 안끝나고, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 tree를 만드는 과정까지가 학습의 과정. <br>\n",
    "그러면, 이제 test는 어떻게?<br>\n",
    "타고 갔는데 순수 노드까지 가면 그 클래스로 판단하면 되지. <br>\n",
    "애매하게 끝난 노드로 가면 어떻게 판단할까?<br>\n",
    "마지막에 끝난 노드가 $class1:class2 = 5:16$이면? class2로 판단. \n",
    "마지막에 끝난 노드가 $class1:class2 = 5:4$이면? class1로 판단. <br>\n",
    "\n",
    "그래서 기본적으로 최종적으로 확률을 구할 수 있음. **확률적 모델**이라고 할 수 있음. <br>\n",
    "다만, 확률 구하는데 베이즈모델 사용 안했지. 그러면 생성모델은 아니고 **확률적 판별모델**인 것. <br>\n",
    "logistic regression이랑 비슷한 놈. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무를 사용한 분류예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 특정 노드로 끝나게 되면, 그 그 노드 안에 *특정클래스 갯수 / 노드 안에 있는 전체 데이터 갯수*로 그 클래스일 확률을 계산한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무에 전체 트레이닝 데이터를 모두 적용해 보면 각 데이터는 특정한 노드를 타고 내려가게 된다. 각 노드는 그 노드를 선택한 데이터 집합을 가진다. 이 때 노드에 속한 데이터의 클래스의 비율을 구하여 이를 그 노드의 조건부 확률 분포 $P(Y=k|X)_{\\text{node}}$라고 정의한다.\n",
    "\n",
    "$$ P(Y=k|X)_{\\text{node}} \\approx \\dfrac{N_{\\text{node},k}}{N_{\\text{node}}} $$\n",
    "\n",
    "테스트 데이터 $X_{\\text{test}}$의 클래스를 예측할 때는 가장 상위의 노드부터 분류 규칙을 차례대로 적용하여 마지막에 도달하는 노드의 조건부 확률 분포를 이용하여 클래스를 예측한다.\n",
    "\n",
    "$$ \\hat{Y} = \\text{arg}\\max_k P(Y=k|X_{\\text{test}})_{\\text{last node}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  분류규칙을 정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 규칙을 정하는 방법은 부모 노드와 자식 노드 간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화한 것이 정보획득량(information gain)이다. 기본적으로 모든 독립 변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획득량이 큰 독립 변수와 기준값을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보획득량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정보획득량(information gain)는 $X$라는 조건에 의해 확률 변수 $Y$의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다. 다음처럼 $Y$의 엔트로피에서 $X$에 대한 $Y$의 조건부 엔트로피를 뺀 값으로 정의된다. \n",
    "\n",
    "$$ IG[Y,X] = H[Y] - H[Y|X] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 A, B 두 가지의 다른 분류 규칙을 적용했더니 다음 처럼 서로 다르게 데이터가 나뉘어 졌다고 가정하자."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_78.png](./materials/1_78.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 43.1 : 분류 결과 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A 방법과 B 방법 모두 노드 분리 전에는 Y=0 인 데이터의 수와 Y=1 인 데이터의 수가 모두 40개였다. \n",
    "\n",
    "A 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.\n",
    "* 자식 노드 A1은 Y=0 인 데이터가 30개, Y=1 인 데이터가 10개 \n",
    "* 자식 노드 A2은 Y=0 인 데이터가 10개, Y=1 인 데이터가 30개 \n",
    "\n",
    "B 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.\n",
    "* 자식 노드 B1은 Y=0 인 데이터가 20개, Y=1 인 데이터가 40개 \n",
    "* 자식 노드 B2은 Y=0 인 데이터가 20개, Y=1 인 데이터가 0개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 부모 노드의 엔트로피를 계산하면 다음과 같다.\n",
    "\n",
    "\n",
    "$$ H[Y] = -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{2} + \\dfrac{1}{2}  = 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 방법에 대해 IG를 계산하면 다음과 같다.\n",
    "\n",
    "$$ H[Y|X=X_1] = -\\dfrac{3}{4}\\log_2\\left(\\dfrac{3}{4}\\right) -\\dfrac{1}{4}\\log_2\\left(\\dfrac{1}{4}\\right) = 0.81 $$\n",
    "\n",
    "$$ H[Y|X=X_2] = -\\dfrac{1}{4}\\log_2\\left(\\dfrac{1}{4}\\right)  -\\dfrac{3}{4}\\log_2\\left(\\dfrac{3}{4}\\right) = 0.81 $$\n",
    "\n",
    "$$ H[Y|X] = \\dfrac{1}{2} H[Y|X=X_1] + \\dfrac{1}{2} H[Y|X=X_2] = 0.81 $$\n",
    "\n",
    "$$ IG = H[Y] - H[Y|X] = 0.19 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B 방법에 대해 IG를 계산하면 다음과 같다.\n",
    "\n",
    "$$ H[Y|X=X_1] = -\\dfrac{1}{3}\\log_2\\left(\\dfrac{1}{3}\\right) - \\dfrac{2}{3}\\log_2\\left(\\dfrac{2}{3}\\right) = 0.92 $$\n",
    "\n",
    "$$ H[Y|X=X_2] = 0 $$\n",
    "\n",
    "$$ H[Y|X] = \\dfrac{3}{4} H[Y|X=X_1] + \\dfrac{1}{4} H[Y|X=X_2] = 0.69 $$\n",
    "\n",
    "$$ IG = H[D] - H[Y|X] = 0.31 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 B 방법이 더 나은 방법임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn의 의사결정나무 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn에서 의사결정나무는 `DecisionTreeClassifier` 클래스로 구현되어있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "y = data.target\n",
    "X = data.data[:, 2:]\n",
    "feature_names = data.feature_names[2:]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree1 = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 의사결정나무를 시각화하기 위한 코드이다. `draw_decision_tree` 함수는 의사결정나무의 의사 결정 과정의 세부적인 내역을 다이어그램으로 보여주고  `plot_decision_regions` 함수는 이러한 의사 결정에 의해 데이터의 영역이 어떻게 나뉘어졌는지를 시각화하여 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pydot\n",
    "from IPython.core.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "def draw_decision_tree(model):\n",
    "    dot_buf = io.StringIO()\n",
    "    export_graphviz(model, out_file=dot_buf, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dot_buf.getvalue())[0]\n",
    "    image = graph.create_png()\n",
    "    return Image(image)\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, model, title):\n",
    "    resolution = 0.01\n",
    "    markers = ('s', '^', 'o')\n",
    "    colors = ('red', 'blue', 'lightgreen')\n",
    "    cmap = mpl.colors.ListedColormap(colors)\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = model.predict(\n",
    "        np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)\n",
    "\n",
    "    plt.contour(xx1, xx2, Z, cmap=mpl.colors.ListedColormap(['k']))\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8,\n",
    "                    c=[cmap(idx)], marker=markers[idx], s=80, label=cl)\n",
    "\n",
    "    plt.xlabel(data.feature_names[2])\n",
    "    plt.ylabel(data.feature_names[3])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, tree1, \"Depth 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y, tree1.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(\n",
    "    criterion='entropy', max_depth=2, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, tree2, \"Depth 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, tree2.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3 = DecisionTreeClassifier(\n",
    "    criterion='entropy', max_depth=3, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(tree3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, tree3, \"Depth 3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, tree3.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree4 = DecisionTreeClassifier(\n",
    "    criterion='entropy', max_depth=4, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, tree4, \"Depth 4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, tree4.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree5 = DecisionTreeClassifier(\n",
    "    criterion='entropy', max_depth=5, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_tree(tree5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, tree5, \"Depth 5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, tree5.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습 문제 1\n",
    "\n",
    "1. 붓꽃 분류 문제에서 꽃받침의 길이와 폭(sepal length, sepal width)을 사용하여 `max_depth=3`인  의사결정나무 모형을 만들고 정확도(accuracy)를 계산하라.\n",
    "2. K=5 인 교차 검증을 통해 테스트 성능 평균을 측정하라.\n",
    "3. `max_depth` 인수를 바꾸어 가면서 테스트 성능 평균을 구하여 cross validation curve를 그리고 가장 테스트 성능 평균이 좋은 `max_depth` 인수를 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X1 = iris.data\n",
    "y1 = iris.target\n",
    "X1 = X1[:, :2]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth=3).fit(X1, y1)\n",
    "y1_pred = model1.predict(X1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y1, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(model1, X1, y1, scoring=\"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_accuracy = []\n",
    "train_accuracy = []\n",
    "for max_depth in np.arange(3, 10):\n",
    "    model1 = DecisionTreeClassifier(max_depth=max_depth).fit(X1, y1)\n",
    "    train_accuracy.append(accuracy_score(y1, model1.predict(X1)))\n",
    "    mean_test_accuracy.append(cross_val_score(model1, X1, y1, scoring=\"accuracy\", cv=5).mean())\n",
    "    \n",
    "    \n",
    "plt.plot(np.arange(3, 10), train_accuracy)\n",
    "plt.plot(np.arange(3, 10), mean_test_accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 타이타닉호 생존자 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"titanic\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"pclass\", \"age\", \"sex\"]\n",
    "dfX = df[feature_names].copy()\n",
    "dfy = df[\"survived\"].copy()\n",
    "dfX.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "dfX[\"sex\"] = LabelEncoder().fit_transform(dfX[\"sex\"])\n",
    "dfX.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX[\"age\"].fillna(dfX[\"age\"].mean(), inplace=True)\n",
    "dfX.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "dfX2 = pd.DataFrame(LabelBinarizer().fit_transform(dfX[\"pclass\"]),\n",
    "                    columns=['c1', 'c2', 'c3'], index=dfX.index)\n",
    "dfX = pd.concat([dfX, dfX2], axis=1)\n",
    "del(dfX[\"pclass\"])\n",
    "dfX.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion='entropy', max_depth=3, min_samples_leaf=5).fit(X_train, y_train)\n",
    "\n",
    "command_buf = io.StringIO()\n",
    "export_graphviz(model, out_file=command_buf, feature_names=[\n",
    "                'Age', 'Sex', '1st_class', '2nd_class', '3rd_class'])\n",
    "graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]\n",
    "image = graph.create_png()\n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습 문제 2\n",
    "\n",
    "1. breast cancer 분류 문제를 의사결정나무를 사용하여 풀어라. K=5인 교차 검증을 하였을 때 평균 성능을 구하라.\n",
    "2. 모든 데이터를 학습 데이터로 사용하였을 때 첫번째로 선택되는 기준은 무엇인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bcancer = load_breast_cancer()\n",
    "X2 = bcancer.data\n",
    "y2 = bcancer.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model2 = DecisionTreeClassifier().fit(X2, y2)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(model2, X2, y2, scoring=\"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(model2, max_depth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습 문제 3\n",
    "\n",
    "1. MINIST digit 이미지 분류 문제를 의사결정나무를 사용하여 풀어라. K=5인 교차 검증을 하였을 때 평균 성능을 구하라.\n",
    "2. 모든 데이터를 학습 데이터로 사용하였을 때 첫번째로 선택되는 픽셀은 어디인가? 이 픽셀은 숫자들을 어떻게 구분하게 되며 왜 그렇게 구분지어지는지 생각하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X3 = digits.data\n",
    "y3 = digits.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model3 = DecisionTreeClassifier().fit(X3, y3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(model3, X3, y3, scoring=\"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(model3, max_depth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy 의사 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의사결정나무의 문제점 중 하나는 특징의 선택이 greedy한 방식으로 이루어지기 때문에 선택된 특징이 최적의 선택이 아닐 수도 있다는 점이다. 예를 들어 데이터가 다음과 같다고 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "y = [0,0,1,1,1,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 노드에서는 $x_1, x_2, x_3$의  성능이 같다. 만약 첫 노드에서 특징으로 $x_1$을 선택하면 2단계로 완벽한 분류를 하는 것이 불가능하다. 그런데 첫 특징으로 $x_1$이 아니라 $x_3$를 선택하면 2번째 단계에서 $x_2$를 선택함으로써 2단계만에 완벽한 분류를 할 수 없다. 하지만 이후의 상황을 첫 노드에서 특징을 결정할 때는 알 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=2).fit(X, y)\n",
    "command_buf = io.StringIO()\n",
    "export_graphviz(model, out_file=command_buf, \n",
    "                feature_names=[\"X1\", \"X2\", \"X3\"])\n",
    "graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]\n",
    "image = graph.create_png()\n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=3).fit(X, y)\n",
    "command_buf = io.StringIO()\n",
    "export_graphviz(model, out_file=command_buf, \n",
    "                feature_names=[\"X1\", \"X2\", \"X3\"])\n",
    "graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]\n",
    "image = graph.create_png()\n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀 나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측값 $\\hat{y}$을 다음처럼 각 특징값 영역마다 고정된 값 $y_1, y_2$를 사용하고,\n",
    "\n",
    "$$ \n",
    "\\hat{y} = \n",
    "\\begin{cases} \n",
    "y_1 & \\text{ if } x \\geq x_{\\text{threshold}} \\\\ \n",
    "y_2 & \\text{ if } x < x_{\\text{threshold}}\n",
    "\\end{cases} $$\n",
    "\n",
    "기준값 및 $y_1, y_2$를 선택하는 목적함수로 오차 제곱합을 사용하면 회귀분석를 할 수 있다. 이러한 모형을 회귀 나무(regression tree)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "regtree = DecisionTreeRegressor(max_depth=3)\n",
    "regtree.fit(X, y)\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_hat = regtree.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"데이터\")\n",
    "plt.plot(X_test, y_hat, color=\"cornflowerblue\", linewidth=2, label=\"예측\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$y$ & $\\hat{y}$\")\n",
    "plt.title(\"회귀 나무\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
