{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 모형최적화 \n",
    "2. 비대칭 데이터 문제\n",
    "3. 특징 선택 \n",
    "4. 대규모 데이터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신 러닝 모형이 완성된 후에는 최적화 과정을 통해 예측 성능을 향상시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn 의 모형 하이퍼 파라미터 튜닝 도구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn에서는 다음과 같은 모형 최적화 도구를 지원한다.\n",
    "\n",
    "* [`validation_curve`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)\n",
    "  * 단일 하이퍼 파라미터 최적화\n",
    "* [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "  * 그리드를 사용한 복수 하이퍼 파라미터 최적화\n",
    "* [`ParameterGrid`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html)  \n",
    " * 복수 파라미터 최적화용 그리드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `validation_curve` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validation_curve` 함수는 최적화할 파라미터 이름과 범위, 그리고 성능 기준을 `param_name`, `param_range`, `scoring` 인수로 받아 파라미터 범위의 모든 경우에 대해 성능 기준을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "param_range = np.logspace(-6, -1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(SVC(), X, y,\n",
    "                     param_name=\"gamma\", param_range=param_range,\n",
    "                     cv=10, scoring=\"accuracy\", n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "mpl.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.semilogx(param_range, test_scores_mean,\n",
    "             label=\"Cross-validation score\", color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV`  클래스는 `validation_curve` 함수와 달리 모형 래퍼(Wrapper) 성격의 클래스이다.  클래스 객체에 `fit` 메서드를 호출하면 grid search를 사용하여 자동으로 복수개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾아준다. 생성된 복수개와 내부 모형과 실행 결과는 다음 속성에 저장된다.\n",
    "\n",
    "* `grid_scores_ `\n",
    " * param_grid 의 모든 파리미터 조합에 대한 성능 결과. 각각의 원소는 다음 요소로 이루어진 튜플이다.\n",
    "  * parameters: 사용된 파라미터\n",
    "  * mean_validation_score: 교차 검증(cross-validation) 결과의 평균값\n",
    "  * cv_validation_scores: 모든 교차 검증(cross-validation) 결과\n",
    "* `best_score_` \n",
    " * 최고 점수 \n",
    "* `best_params_`\n",
    " * 최고 점수를 낸 파라미터\n",
    "* `best_estimator_`\n",
    " * 최고 점수를 낸 파라미터를 가진 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [\n",
    "    {'clf__C': param_range, 'clf__kernel': ['linear']},\n",
    "    {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid,\n",
    "                  scoring='accuracy', cv=10, n_jobs=1)\n",
    "%time gs = gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ParameterGrid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "때로는 scikit-learn 이 제공하는 GridSearchCV 이외의 방법으로 그리드 탐색을 해야하는 경우도 있다. 이 경우 파라미터를 조합하여 탐색 그리드를 생성해 주는 명령어가 `ParameterGrid` 이다. `ParameterGrid` 는 탐색을 위한 iterator 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
    "list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` 명령에는 `n_jobs` 라는 인수가 있다. 디폴트 값은 1인데 이 값을 증가시키면 내부적으로 멀티 프로세스를 사용하여 그리드서치를 수행한다. 만약 CPU 코어의 수가 충분하다면 `n_jobs`를 늘릴 수록 속도가 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"gamma\": np.logspace(-6, -1, 10)}\n",
    "gs1 = GridSearchCV(estimator=SVC(), param_grid=param_grid,\n",
    "                   scoring='accuracy', cv=5, n_jobs=1)\n",
    "gs2 = GridSearchCV(estimator=SVC(), param_grid=param_grid,\n",
    "                   scoring='accuracy', cv=5, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 하드웨어의 코어 수가 부족하다면 병렬로 실행되지 않으므로 실행시간이 단축되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비대칭 데이터 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 클래스 비율이 너무 차이가 나면(highly-imbalanced data) 단순히 우세한 클래스를 택하는 모형의 정확도가 높아지므로 모형의 성능판별이 어려워진다. 즉, 정확도(accuracy)가 높아도 데이터 갯수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상이 발생할 수 있다.\n",
    "\n",
    "이렇게 각 클래스에 속한 데이터의 갯수의 차이에 의해 발생하는 문제들을 비대칭 데이터 문제(imbalanced data problem)이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def classification_result(n0, n1, title=\"\"):\n",
    "    rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])\n",
    "    rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])\n",
    "    X0 = rv1.rvs(n0, random_state=0)\n",
    "    X1 = rv2.rvs(n1, random_state=0)\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n0), np.ones(n1)])\n",
    "\n",
    "    x1min = -4; x1max = 4\n",
    "    x2min = -2; x2max = 2\n",
    "    xx1 = np.linspace(x1min, x1max, 1000)\n",
    "    xx2 = np.linspace(x2min, x2max, 1000)\n",
    "    X1, X2 = np.meshgrid(xx1, xx2)\n",
    "\n",
    "    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=\"dashed\")\n",
    "    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=\"dashed\")\n",
    "\n",
    "    model = SVC(kernel=\"linear\", C=1e4, random_state=0).fit(X, y)\n",
    "    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='x', label=\"0 클래스\")\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='o', label=\"1 클래스\")\n",
    "    plt.contour(X1, X2, Y, colors='k', levels=[0.5])\n",
    "    y_pred = model.predict(X)\n",
    "    plt.xlim(-4, 4)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    return model, X, y, y_pred\n",
    "    \n",
    "plt.subplot(121)\n",
    "model1, X1, y1, y_pred1 = classification_result(200, 200, \"대칭 데이터 (5:5)\")\n",
    "plt.subplot(122)\n",
    "model2, X2, y2, y_pred2 = classification_result(200, 20, \"비대칭 데이터 (9:1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y1, y_pred1))\n",
    "print(classification_report(y2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y1, model1.decision_function(X1))\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y2, model2.decision_function(X2))\n",
    "\n",
    "c1 = confusion_matrix(y1, y_pred1, labels=[1, 0])\n",
    "c2 = confusion_matrix(y2, y_pred2, labels=[1, 0])\n",
    "r1 = c1[0, 0] / (c1[0, 0] + c1[0, 1])\n",
    "r2 = c2[0, 0] / (c2[0, 0] + c2[0, 1])\n",
    "f1 = c1[1, 0] / (c1[1, 0] + c1[1, 1])\n",
    "f2 = c2[1, 0] / (c2[1, 0] + c2[1, 1])\n",
    "\n",
    "plt.plot(fpr1, tpr1, ':', label=\"대칭\")\n",
    "plt.plot(fpr2, tpr2, '-', label=\"비대칭\")\n",
    "plt.plot([f1], [r1], 'ro')\n",
    "plt.plot([f2], [r2], 'ro')\n",
    "plt.legend()\n",
    "plt.xlabel('Fall-Out')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('ROC 커브')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비대칭 데이터는 다수 클래스 데이터에서 일부만 사용하는 **언더 샘플링**이나 소수 클래스 데이터를 증가시키는 **오버 샘플링**을 사용하여 데이터 비율을 맞추면 정밀도(precision)가 향상된다.\n",
    "\n",
    "* 오버샘플링(Over-Sampling)\n",
    "* 언더샘플링(Under-Sampling)\n",
    "* 복합샘플링(Combining Over-and Under-Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imbalanced-learn 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imbalanced data 문제를 해결하기 위한 다양한 샘플링 방법을 구현한 파이썬 패키지\n",
    "\n",
    "\n",
    "```\n",
    "pip install -U imbalanced-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언더 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `RandomUnderSampler`: random under-sampling method\n",
    "* `TomekLinks`: Tomek’s link method\n",
    "* `CondensedNearestNeighbour`: condensed nearest neighbour method\n",
    "* `OneSidedSelection`: under-sampling based on one-sided selection method\n",
    "* `EditedNearestNeighbours`: edited nearest neighbour method\n",
    "* `NeighbourhoodCleaningRule`: neighbourhood cleaning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import *\n",
    "\n",
    "n0 = 200; n1 = 20\n",
    "rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])\n",
    "rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])\n",
    "X0 = rv1.rvs(n0, random_state=0)\n",
    "X1 = rv2.rvs(n1, random_state=0)\n",
    "X_imb = np.vstack([X0, X1])\n",
    "y_imb = np.hstack([np.zeros(n0), np.ones(n1)])\n",
    "\n",
    "x1min = -4; x1max = 4\n",
    "x2min = -2; x2max = 2\n",
    "xx1 = np.linspace(x1min, x1max, 1000)\n",
    "xx2 = np.linspace(x2min, x2max, 1000)\n",
    "X1, X2 = np.meshgrid(xx1, xx2)\n",
    "\n",
    "def classification_result2(X, y, title=\"\"):\n",
    "    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=\"dashed\")\n",
    "    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=\"dashed\")\n",
    "    model = SVC(kernel=\"linear\", C=1e4, random_state=0).fit(X, y)\n",
    "    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='x', label=\"0 클래스\")\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='o', label=\"1 클래스\")\n",
    "    plt.contour(X1, X2, Y, colors='k', levels=[0.5])\n",
    "    y_pred = model.predict(X)\n",
    "    plt.xlim(-4, 4)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramdom Under-Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 무작위로 데이터를 없애는 단순 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomek’s link method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토멕링크(Tomek’s link)란 서로 다른 클래스에 속하는 한 쌍의 데이터 $(x_{+}, x_{-})$로 서로에게 더 가까운 다른 데이터가 존재하지 않는 것이다. 즉 클래스가 다른 두 데이터가 아주 가까이 붙어있으면 토멕링크가 된다. 토멕링크 방법은 이러한 토멕링크를 찾은 다음 그 중에서 다수 클래스에 속하는 데이터를 제외하는  방법으로 경계선을 다수 클래스쪽으로 밀어붙이는 효과가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<picture>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condensed Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN(Condensed Nearest Neighbour) 방법은 1-NN 모형으로 분류되지 않는 데이터만 남기는 방법이다. 선텍된 데이터 집합을 $S$라고 하자.\n",
    "\n",
    "1. 소수 클래스 데이터를 모두 $S$에 포함시킨다.\n",
    "2. 다수 데이터 중에서 하나를 골라서 가장 가까운 데이터가 다수 클래스이면 포함시키지 않고 아니면 $S$에 포함시킨다.\n",
    "3. 더이상 선택되는 데이터가 없을 때까지 2를 반복한다.\n",
    "\n",
    "이 방법을 사용하면 기존에 선택된 데이터와 가까이 있으면서 같은 클래스인 데이터는 선택되지 않기 때문에 다수 데이터의 경우 선택되는 비율이 적어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Sided Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Sided Selection은 토맥링크 방법과 Condensed Nearest Neighbour 방법을 섞은 것이다. 토맥링크 중 다수 클래스를 제외하고 나머지 데이터 중에서도 서로 붙어있는 다수 클래스 데이터는 1-NN 방법으로 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edited Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENN(Edited Nearest Neighbours) 방법은 다수 클래스 데이터 중 가장 가까운 k(`n_neighbors`)개의 데이터가 모두(`kind_sel=\"all\"`) 또는 다수(`kind_sel=\"mode\"`) 다수 클래스가 아니면 삭제하는 방법이다. 소수 클래스 주변의 다수 클래스 데이터는 사라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood Cleaning Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbourhood Cleaning Rule 방법은 CNN(Condensed Nearest Neighbour) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `RandomOverSampler`: random sampler\n",
    "* `ADASYN`: Adaptive Synthetic Sampling Approach for Imbalanced Learning\n",
    "* `SMOTE`: Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Over Sampling은 소수 클래스의 데이터를 반복해서 넣는 것(replacement)이다. 가중치를 증가시키는 것과 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADASYN(Adaptive Synthetic Sampling) 방법은 소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE(Synthetic Minority Over-sampling Technique) 방법도 ADASYN 방법처럼 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<picture>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복합 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `SMOTEENN`: SMOTE + ENN\n",
    "* `SMOTETomek`: SMOTE + Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE+ENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE+ENN 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE+Tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE+Tomek 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 토멕링크 방법을 섞은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)\n",
    "\n",
    "plt.subplot(121)\n",
    "classification_result2(X_imb, y_imb)\n",
    "plt.subplot(122)\n",
    "model_samp = classification_result2(X_samp, y_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_imb, model_samp.predict(X_imb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특징 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실무에서는 대규모의 데이터를 기반으로 분류예측 모형을 만들어야 하는 경우가 많다. 대규모의 데이터라고 하면 표본의 갯수가 많거나 아니면 독립변수 즉, 특징데이터의 종류가 많거나 혹은 이 두가지 모두인 경우가 있다. 여기에서는 특징데이터의 종류가 많은 경우에 가장 중요하다고 생각되는 특징데이터만 선택하여 특징데이터의 종류를 줄이기 위한 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv_train = fetch_rcv1(subset=\"train\")\n",
    "rcv_test = fetch_rcv1(subset=\"test\")\n",
    "X_train = rcv_train.data\n",
    "y_train = rcv_train.target\n",
    "X_test = rcv_test.data\n",
    "y_test = rcv_test.target\n",
    "\n",
    "# Ont-Hot-Encoding된 라벨을 정수형으로 복원\n",
    "classes = np.arange(rcv_train.target.shape[1])\n",
    "y_train = y_train.dot(classes)\n",
    "y_test = y_test.dot(classes)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산에 의한 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 예측모형에서 중요한 특징데이터란 종속데이터와의 상관관계가 크고 예측에 도움이 되는 데이터를 말한다. 하지만 상관관계 계산에 앞서 특징데이터의 값 자체가 표본에 따라 그다지 변하지 않는다면 종속데이터 예측에도 도움이 되지 않을 가능성이 높다. 따라서 표본 변화에 따른 데이터 값의 변화 즉, 분산이 기준치보다 낮은 특징 데이터는 사용하지 않는 방법이 분산에 의한 선택 방법이다.\n",
    "예를 들어 종속데이터와 특징데이터가 모두 0 또는 1 두가지 값만 가지는데 종속데이터는 0과 1이 균형을 이루는데 반해 특징데이터가 대부분(예를 들어 90%)의 값이 0이라면 이 특징데이터는 분류에 도움이 되지 않을 가능성이 높다.\n",
    "\n",
    "하지만 분산에 의한 선택은 반드시 상관관계와 일치한다는 보장이 없기 때문에 신중하게 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(1e-5)\n",
    "X_train_sel = selector.fit_transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "X_train_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"train accuracy:{:5.3f}\".format(accuracy_score(y_train, model.predict(X_train))))\n",
    "print(\"test accuracy :{:5.3f}\".format(accuracy_score(y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_sel, y_train)\n",
    "print(\"train accuracy:{:5.3f}\".format(accuracy_score(y_train, model.predict(X_train_sel))))\n",
    "print(\"test accuracy :{:5.3f}\".format(accuracy_score(y_test, model.predict(X_test_sel))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단일 변수 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단일 변수 선택법은 각각의 독립변수를 하나만 사용한 예측모형의 성능을 이용하여 가장 분류성능 혹은 상관관계가 높은 변수만 선택하는 방법이다. 사이킷런 패키지의 feature_selection 서브패키지는 다음 성능지표를 제공한다.\n",
    "\n",
    "* `chi2`: 카이제곱 검정 통계값\n",
    "* `f_classif`: 분산분석(ANOVA) F검정 통계값\n",
    "* `mutual_info_classif`: 상호정보량(mutual information)\n",
    "\n",
    "하지만 단일 변수의 성능이 높은 특징만 모았을 때 전체 성능이 반드시 향상된다는 보장은 없다.\n",
    "\n",
    "feature_selection 서브패키지는 성능이 좋은 변수만 사용하는 전처리기인 `SelectKBest` 클래스도 제공한다. 사용법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "selector1 = SelectKBest(chi2, k=14330)\n",
    "X_train1 = selector1.fit_transform(X_train, y_train)\n",
    "X_test1 = selector1.transform(X_test)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train1, y_train)\n",
    "print(\"train accuracy:{:5.3f}\".format(accuracy_score(y_train, model.predict(X_train1))))\n",
    "print(\"test accuracy :{:5.3f}\".format(accuracy_score(y_test, model.predict(X_test1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 모형을 이용한 특성 중요도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특성 중요도(feature importance)를 계산할 수 있는 랜덤포레스트 등의 다른 모형을 사용하여 일단 특성을 선택하고 최종 분류는 다른 모형을 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_sample = 10000\n",
    "idx = np.random.choice(range(len(y_train)), n_sample)\n",
    "model_sel = ExtraTreesClassifier(n_estimators=50).fit(X_train[idx, :], y_train[idx])\n",
    "selector = SelectFromModel(model_sel, prefit=True, max_features=14330)\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_sel, y_train)\n",
    "print(\"train accuracy:{:5.3f}\".format(accuracy_score(y_train, model.predict(X_train_sel))))\n",
    "print(\"test accuracy :{:5.3f}\".format(accuracy_score(y_test, model.predict(X_test_sel))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대규모 데이터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대규모 데이터(big data)의 경우에는 메모리 등의 문제로 특정한 모형은 사용할 수 없는 경우가 많다. 이 때는 \n",
    "\n",
    "* 사전 확률분포를 설정할 수 있는 생성 모형\n",
    "* 시작 가중치를 설정할 수 있는 모형\n",
    "\n",
    "등을 이용하고 전체 데이터를 처리 가능한 작은 주각으로 나누어 학습을 시키는 점진적 학습 방법을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "covtype = fetch_covtype(shuffle=True, random_state=0)\n",
    "X_covtype = covtype.data\n",
    "y_covtype = covtype.target - 1\n",
    "classes = np.unique(y_covtype)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_covtype, y_covtype)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def read_Xy(start, end):\n",
    "    # 실무에서는 파일이나 데이터베이스에서 읽어온다.\n",
    "    idx = list(range(start, min(len(y_train) - 1, end)))\n",
    "    X = X_train[idx, :]\n",
    "    y = y_train[idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론 모형은 가중치를 계속 업데이트하므로 일부 데이터를 사용하여 구한 가중치를 다음 단계에서 초기 가중치로 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SGDClassifier(random_state=0)\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    for n in range(n_split):\n",
    "        X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "        model.partial_fit(X, y, classes=classes)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train acc={:5.3f} test acc={:5.3f}\".format(epoch, accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브베이즈 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브베이즈 모형과 같은 생성모형은 일부 데이터를 이용하여 구한 확률분포를 사전확률분포로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = BernoulliNB(alpha=0.1)\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test)) \n",
    "    print(\"n={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레디언트 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레디언트 부스팅에서는 초기 커미티 멤버로 일부 데이터를 사용하여 학습한 모형을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lightgbm import train, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    \"num_class\": len(classes),\n",
    "    'learning_rate': 0.2,\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree = 10\n",
    "model = None\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model = train(params, init_model=model, train_set=Dataset(X, y),\n",
    "                  keep_training_booster=False, num_boost_round=num_tree)\n",
    "    accuracy_train = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
    "    accuracy_test = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1)) \n",
    "    print(\"n={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 포레스트와 같은 앙상블 모형에서는 일부 데이터를 사용한 모형을 개별 분류기로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree_ini = 10\n",
    "num_tree_step = 10\n",
    "model = RandomForestClassifier(n_estimators=num_tree_ini, warm_start=True)\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model.fit(X, y)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))\n",
    "    \n",
    "    model.n_estimators += num_tree_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
